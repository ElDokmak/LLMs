{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q -U transformers peft accelerate optimum","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:49:45.377107Z","iopub.execute_input":"2023-09-10T14:49:45.377987Z","iopub.status.idle":"2023-09-10T14:50:08.964655Z","shell.execute_reply.started":"2023-09-10T14:49:45.377939Z","shell.execute_reply":"2023-09-10T14:50:08.963184Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:50:08.971076Z","iopub.execute_input":"2023-09-10T14:50:08.974557Z","iopub.status.idle":"2023-09-10T14:50:21.807755Z","shell.execute_reply.started":"2023-09-10T14:50:08.974513Z","shell.execute_reply":"2023-09-10T14:50:21.806609Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu117/\nCollecting auto-gptq\n  Downloading https://huggingface.github.io/autogptq-index/whl/cu117/auto-gptq/auto_gptq-0.4.2%2Bcu117-cp310-cp310-linux_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.22.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.23.5)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.3.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.33.1)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.5.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.19.0->auto-gptq) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (4.66.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.0.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.19.0->auto-gptq) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nInstalling collected packages: rouge, auto-gptq\nSuccessfully installed auto-gptq-0.4.2+cu117 rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Quantization using Auto-gptq","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:52:38.043887Z","iopub.execute_input":"2023-09-10T14:52:38.044279Z","iopub.status.idle":"2023-09-10T14:52:38.064556Z","shell.execute_reply.started":"2023-09-10T14:52:38.044226Z","shell.execute_reply":"2023-09-10T14:52:38.063680Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"For quantizing a model using auto-gptq, we need to pass a dataset to the quantizer. \n\nThis can be achieved either by passing a supported default dataset among ['wikitext2','c4','c4-new','ptb','ptb-new'] or a list of strings that will be used as a dataset.","metadata":{}},{"cell_type":"code","source":"model_id = \"facebook/opt-125m\"\nquantization_config = GPTQConfig(\n    bits = 4,\n    group_size = 128,\n    dataset = \"c4\",\n    desc_act=False,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = quantization_config,\n    device_map = 'auto'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T14:57:13.443075Z","iopub.execute_input":"2023-09-10T14:57:13.443473Z","iopub.status.idle":"2023-09-10T15:00:51.906849Z","shell.execute_reply.started":"2023-09-10T14:57:13.443439Z","shell.execute_reply":"2023-09-10T15:00:51.905770Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055e7a4e52f344e1a3b8954655ee8f00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466bee34e509491ba53423f3c5e2e3fa"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset json/allenai--c4 to /root/.cache/huggingface/datasets/json/allenai--c4-6fbe877195f42de5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e3610a0e6de45258b6eebb4c5a4b5f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbbccaab70ef4a2298f05b5df758566e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b1193299ef4a3cba8cb48ad5da1fd4"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/allenai--c4-6fbe877195f42de5/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9515fffa6f403c9467536197b07d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"markdown","source":"* To make sure the model has been correctly quantized run the following line of code \n* qzeros and qweights both should be int32","metadata":{}},{"cell_type":"code","source":"model.model.decoder.layers[0].self_attn.q_proj.__dict__","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:01:10.836353Z","iopub.execute_input":"2023-09-10T15:01:10.837544Z","iopub.status.idle":"2023-09-10T15:01:10.920826Z","shell.execute_reply.started":"2023-09-10T15:01:10.837488Z","shell.execute_reply":"2023-09-10T15:01:10.919919Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'training': True,\n '_parameters': OrderedDict(),\n '_buffers': OrderedDict([('qweight',\n               tensor([[ 1711760090, -1248295259, -2025411892,  ..., -1486452502,\n                         2019142072, -1735820810],\n                       [-2000132747,  -578262345,  1484081337,  ..., -1230600537,\n                        -2019252040, -2023311003],\n                       [ -710293851, -1153090188,  1431922298,  ..., -1768449094,\n                         2042194587, -2004125258],\n                       ...,\n                       [-1183500136, -1494510422, -1772782904,  ..., -1518753378,\n                         -411710600,  -392845654],\n                       [-1990626701,  1469278281,  1469864108,  ...,  1740208533,\n                        -1732560507, -1738077576],\n                       [ 2015914598,  2040232821,  2005572185,  ..., -1463179655,\n                        -1450400136, -2024523156]], device='cuda:0', dtype=torch.int32)),\n              ('qzeros',\n               tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n                      device='cuda:0', dtype=torch.int32)),\n              ('scales',\n               tensor([[0.0472, 0.0420, 0.0370,  ..., 0.0180, 0.0131, 0.0191],\n                       [0.0506, 0.0483, 0.0370,  ..., 0.0103, 0.0141, 0.0115],\n                       [0.0418, 0.0524, 0.0418,  ..., 0.0190, 0.0142, 0.0217],\n                       [0.0481, 0.0362, 0.0438,  ..., 0.0124, 0.0149, 0.0118],\n                       [0.0437, 0.0489, 0.0376,  ..., 0.0199, 0.0162, 0.0143],\n                       [0.0474, 0.0491, 0.0381,  ..., 0.0144, 0.0144, 0.0163]],\n                      device='cuda:0', dtype=torch.float16)),\n              ('g_idx',\n               tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n                       3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n                      device='cuda:0', dtype=torch.int32)),\n              ('bias',\n               tensor([-1.4294e-01, -3.6646e-01, -2.9221e-02, -2.8613e-01, -3.0615e-01,\n                       -2.6880e-01,  7.9773e-02,  8.0261e-02, -1.7957e-01, -5.2887e-02,\n                        1.4319e-01, -1.8982e-01,  3.6694e-01, -6.1554e-02,  3.2617e-01,\n                       -2.8857e-01,  6.8481e-02, -2.7271e-01,  1.5649e-01, -2.1838e-01,\n                       -2.1082e-01, -1.3550e-01, -2.5049e-01, -3.3594e-01,  2.9150e-01,\n                       -7.9773e-02,  1.3440e-01,  8.8196e-02,  3.8110e-01,  1.4294e-01,\n                       -7.5989e-02, -1.9873e-01, -2.1973e-02,  1.3397e-02,  1.1505e-01,\n                        2.5146e-01,  2.8589e-01,  2.9956e-01, -2.2217e-01,  1.3135e-01,\n                       -2.5269e-01, -2.0190e-01,  8.5083e-02, -5.4901e-02,  2.3267e-01,\n                       -1.3831e-01, -5.8624e-02, -1.1206e-01,  5.0488e-01,  1.4526e-01,\n                        5.2295e-01,  2.3157e-01, -1.0779e-01,  2.0374e-01,  1.8323e-01,\n                       -3.3862e-01,  1.9885e-01, -1.6028e-01, -6.7139e-02,  3.3740e-01,\n                        1.3025e-01,  1.1078e-01,  3.5034e-01, -2.9590e-01,  5.2539e-01,\n                        2.8003e-01, -3.3691e-02,  3.4546e-01,  1.3342e-01,  1.0510e-01,\n                       -1.2433e-01,  2.7686e-01, -4.4098e-02,  6.0921e-03,  1.0352e-01,\n                        5.0635e-01, -1.7041e-01,  5.0391e-01, -3.5010e-01,  5.4590e-01,\n                       -1.5039e-01, -1.8030e-01, -1.4600e-01,  3.5498e-01,  1.1182e-01,\n                        2.5659e-01, -3.5352e-01, -3.8379e-01,  5.3174e-01, -2.0215e-01,\n                       -2.5928e-01, -1.6821e-01,  3.0304e-02, -2.7026e-01, -4.4312e-01,\n                       -2.2449e-01,  1.2000e-01,  6.0059e-02,  2.6953e-01,  3.9722e-01,\n                       -5.0195e-01, -4.9927e-02,  6.1920e-02, -1.1469e-01, -1.8988e-03,\n                       -6.6650e-02, -1.6272e-01,  3.7134e-01, -3.2568e-01,  7.4196e-03,\n                        6.5079e-03, -3.2318e-02, -2.0874e-02, -3.4790e-01, -4.3579e-02,\n                        3.3618e-01, -1.6418e-01, -4.5441e-02,  2.7246e-01,  1.1267e-01,\n                       -3.1738e-01, -9.1919e-02, -3.3008e-01,  4.5532e-02,  2.4094e-02,\n                       -2.0227e-01, -1.8384e-01,  9.4910e-02, -5.8167e-02, -2.6343e-01,\n                       -1.4722e-01,  3.2153e-01, -8.6609e-02, -3.0957e-01,  2.8174e-01,\n                       -2.3413e-01,  1.7090e-01,  2.5317e-01,  1.2683e-01, -3.0444e-01,\n                        2.6855e-01,  3.8849e-02,  2.2925e-01,  1.9922e-01,  2.4719e-01,\n                        4.0942e-01,  1.1780e-01,  1.1475e-01, -6.1127e-02,  2.1313e-01,\n                        3.8184e-01, -5.8899e-02,  1.0278e-01,  2.3950e-01,  2.8784e-01,\n                       -3.0457e-02, -1.3049e-01, -3.7646e-01,  2.2119e-01, -1.6406e-01,\n                       -1.9495e-01,  1.7273e-01, -2.5464e-01,  3.6768e-01, -2.9102e-01,\n                       -1.5918e-01,  2.0471e-01, -3.2812e-01,  2.2131e-01, -1.1847e-01,\n                        2.0642e-01,  3.2324e-01, -4.1895e-01, -1.8042e-01, -2.6733e-01,\n                       -2.9834e-01, -2.4695e-01, -2.6465e-01,  2.0676e-02,  6.9763e-02,\n                        1.1255e-01,  3.0957e-01,  5.0293e-01,  3.1421e-01,  1.4185e-01,\n                       -2.6562e-01, -2.4817e-01, -5.6854e-02,  1.8518e-01, -2.5269e-01,\n                       -2.2961e-01, -8.8867e-02, -4.0796e-01, -1.9324e-01,  4.3262e-01,\n                       -3.6523e-01,  4.0016e-03,  1.8518e-01,  2.6416e-01,  2.4414e-01,\n                        9.4177e-02, -1.8005e-01, -1.4246e-01, -2.9395e-01,  3.0396e-01,\n                       -3.4131e-01, -4.0039e-01, -2.5732e-01, -3.2959e-01,  1.6553e-01,\n                        2.8491e-01, -2.8394e-01,  5.0342e-01,  2.0190e-01, -4.0576e-01,\n                        1.8066e-01,  2.8882e-01,  4.3945e-01,  7.6561e-03, -3.0322e-01,\n                        2.8101e-01,  1.8518e-01, -3.6682e-02,  1.5088e-01,  2.3340e-01,\n                       -4.3042e-01, -3.6475e-01, -3.6640e-03, -3.4912e-01, -3.8013e-01,\n                       -4.0436e-02,  3.5474e-01, -4.4434e-01,  5.0830e-01,  3.2373e-01,\n                        2.6807e-01,  2.6416e-01, -4.2505e-01,  2.9956e-01,  2.4902e-01,\n                        2.5269e-01,  3.7646e-01, -4.3091e-01,  3.6304e-01,  1.9238e-01,\n                        1.1406e-02, -9.9258e-03, -2.1826e-01, -4.3396e-02, -3.0566e-01,\n                       -3.7842e-01, -1.7554e-01, -5.0293e-01, -3.7109e-01,  3.6548e-01,\n                       -4.4824e-01,  3.2654e-02, -1.1391e-02,  6.0730e-02,  1.7426e-02,\n                       -1.6431e-01, -8.7219e-02,  1.8884e-01,  7.8552e-02, -4.7821e-02,\n                        9.8724e-03, -1.9165e-02, -9.9945e-03, -1.0797e-01, -4.2084e-02,\n                        3.9635e-03, -2.5732e-01,  1.3252e-02,  7.1289e-02,  1.6113e-02,\n                       -1.3382e-02, -2.1393e-02,  8.0872e-02, -2.5952e-01,  2.6932e-02,\n                       -1.4763e-02, -2.5284e-02, -2.7905e-01, -3.7659e-02, -1.9409e-02,\n                       -8.9050e-02, -9.7733e-03, -6.6223e-02,  3.5400e-02, -7.4654e-03,\n                        6.4697e-02,  1.6052e-01, -1.9073e-02, -1.9806e-02,  3.6835e-02,\n                        6.1737e-02, -1.0675e-01,  1.5793e-02,  1.3147e-01,  2.5772e-02,\n                        3.2501e-02,  3.2440e-02, -4.0619e-02,  4.0359e-03, -5.0140e-02,\n                       -7.7087e-02,  1.2772e-02,  7.0862e-02, -5.2246e-02,  5.9509e-04,\n                       -2.0569e-02, -4.2999e-02,  1.4465e-02, -2.6562e-01,  5.4359e-03,\n                       -9.1858e-02, -6.3171e-02,  1.6586e-02,  4.0649e-02,  8.2214e-02,\n                        2.0844e-02,  5.9967e-03,  4.3488e-02,  2.6047e-02, -2.3384e-03,\n                       -1.2207e-02, -2.7603e-02,  5.6250e-01, -1.6510e-02, -1.4435e-02,\n                        5.2930e-01,  2.1805e-02, -1.0262e-03, -2.6810e-02, -2.4414e-03,\n                       -2.5978e-03,  5.5615e-01, -1.9653e-02, -3.8357e-03, -1.4435e-02,\n                       -1.5656e-02, -5.8441e-02, -1.7746e-02, -2.8870e-02,  4.0314e-02,\n                        7.3671e-04,  2.7588e-02, -2.4231e-02,  1.5434e-02,  7.2289e-03,\n                       -1.5221e-02,  3.0075e-02,  1.4343e-02,  1.7609e-02, -7.1869e-03,\n                       -2.0630e-02, -3.5736e-02,  3.8574e-02,  6.7139e-04,  1.4320e-02,\n                        4.2343e-03, -3.5156e-02,  2.5558e-02,  4.6659e-04, -1.2123e-02,\n                       -2.3880e-02,  1.8906e-02,  7.2937e-03, -2.8259e-02,  1.5465e-02,\n                       -5.1074e-01,  3.4149e-02,  1.1234e-03,  1.4786e-02,  1.7639e-02,\n                        5.1849e-02, -6.3591e-03,  4.0741e-03, -2.8381e-03, -2.1534e-03,\n                       -3.2532e-02,  4.7424e-02,  1.7365e-02,  5.9853e-03,  1.7651e-01,\n                       -6.3477e-02, -3.6133e-02, -1.9006e-01,  2.5586e-01, -3.6938e-01,\n                       -3.5034e-01, -1.9250e-01, -3.6938e-01, -1.6724e-01,  3.4619e-01,\n                        2.4377e-01,  1.3721e-01,  1.8158e-02,  1.2109e-01,  3.1128e-01,\n                       -4.5228e-04,  1.8616e-01,  4.4580e-01,  2.2864e-01, -3.3661e-02,\n                       -2.1655e-01,  4.4507e-01, -1.2964e-01, -1.9812e-01, -2.3779e-01,\n                       -2.4524e-01,  2.4585e-01, -2.5439e-01,  4.7095e-01, -6.5308e-02,\n                       -3.3765e-01, -4.6045e-01, -1.1108e-01,  2.8442e-01,  1.3000e-01,\n                        5.0342e-01,  1.7236e-01,  1.1676e-01,  1.6870e-01,  3.2983e-01,\n                       -2.1204e-01,  1.1945e-01,  9.6313e-02,  1.2390e-01, -3.0811e-01,\n                        3.0981e-01, -7.5623e-02,  1.7664e-01,  3.6572e-01,  4.6582e-01,\n                        1.7114e-01, -2.4744e-01,  7.0496e-02, -3.2837e-02,  3.2568e-01,\n                        1.4648e-01, -2.9785e-01, -1.1273e-01,  1.7529e-01, -3.9429e-01,\n                       -1.6736e-01, -3.0609e-02, -4.4824e-01, -6.6284e-02,  1.2543e-02,\n                       -1.6614e-01,  2.9614e-01, -3.0859e-01, -1.7639e-01,  8.0322e-02,\n                       -3.1348e-01, -2.6514e-01, -2.7295e-01, -2.3755e-01,  1.0217e-01,\n                        8.8379e-02,  8.1177e-02,  3.8391e-02, -5.9509e-02,  3.3325e-01,\n                       -7.1594e-02, -7.3669e-02,  2.3544e-02,  4.9347e-02,  1.7932e-01,\n                        1.3123e-01,  1.7517e-01, -3.6499e-01,  2.6733e-01, -2.6245e-01,\n                       -8.0490e-03,  7.1487e-03, -2.1204e-01,  5.1123e-01, -1.5356e-01,\n                       -4.1382e-01, -1.7810e-01, -6.2866e-02,  1.1420e-01,  8.2520e-02,\n                       -3.1641e-01, -1.5335e-02, -5.0995e-02, -2.9199e-01, -1.7847e-01,\n                        4.2145e-02,  2.9663e-01, -7.2144e-02, -1.3220e-01,  3.0322e-01,\n                       -1.2683e-01, -2.8961e-02,  2.2430e-02,  7.2876e-02,  7.6599e-02,\n                       -2.7145e-02,  2.5854e-01, -1.2128e-01, -3.8867e-01, -6.0699e-02,\n                        2.7786e-02, -3.5059e-01,  6.4636e-02,  1.3660e-01,  2.9395e-01,\n                       -1.2329e-01, -8.0444e-02, -9.2163e-02, -5.5518e-01,  1.5649e-01,\n                       -5.0732e-01, -5.2539e-01,  5.5127e-01, -2.2064e-02, -6.2158e-01,\n                        5.0586e-01, -5.2490e-01,  1.0199e-01, -2.4683e-01,  5.1807e-01,\n                        3.6499e-01, -5.5615e-01,  5.6299e-01, -1.1469e-01, -5.1172e-01,\n                        1.5137e-01,  5.5811e-01, -5.7861e-01, -5.0928e-01,  3.2617e-01,\n                        2.3938e-01, -5.7666e-01,  1.6016e-01, -2.5488e-01, -3.4863e-01,\n                        2.4780e-01,  3.9337e-02, -1.6858e-01, -8.2825e-02, -3.3325e-02,\n                        5.0146e-01,  2.9688e-01,  1.3110e-01,  5.7812e-01,  2.0154e-01,\n                        8.7585e-02, -5.1416e-01, -2.4902e-01, -1.6833e-01,  5.3760e-01,\n                       -5.3857e-01,  5.6915e-02,  5.0195e-01,  1.3464e-01,  5.0586e-01,\n                        6.7383e-02,  1.7346e-01, -2.5781e-01, -5.3857e-01,  7.5455e-03,\n                       -5.0781e-01, -3.9404e-01,  3.0176e-01,  5.0977e-01, -1.2421e-01,\n                       -5.0293e-01, -1.2347e-01,  5.1416e-01, -5.3320e-01, -5.7080e-01,\n                       -3.4644e-01,  5.0781e-01, -7.8796e-02, -4.2236e-02, -2.6221e-01,\n                        1.0114e-01,  1.0971e-02, -7.5562e-02, -2.2595e-01, -1.4502e-01,\n                        3.7671e-01,  3.6938e-01,  1.5576e-01,  5.8632e-03,  1.6602e-01,\n                        2.8955e-01, -4.7217e-01, -1.1194e-01,  2.9077e-01,  2.6758e-01,\n                       -6.3293e-02,  1.1340e-01,  5.3174e-01,  4.0210e-01,  1.3965e-01,\n                        1.1505e-01,  8.7891e-02, -8.8562e-02, -4.9023e-01,  3.9380e-01,\n                        1.3611e-01,  2.3767e-01,  1.9424e-02, -5.5322e-01,  2.3218e-01,\n                       -5.2881e-01, -1.0211e-01,  2.5854e-01, -3.2520e-01,  1.6632e-02,\n                       -2.7979e-01,  2.8857e-01, -7.1289e-02, -5.2979e-01,  2.5195e-01,\n                       -3.1226e-01, -2.7466e-03,  1.2793e-01,  9.7168e-02,  6.2103e-02,\n                        9.3567e-02,  2.4023e-01,  4.6069e-01, -5.0926e-03,  2.7271e-01,\n                        1.3184e-01, -2.3901e-01, -3.5205e-01, -1.8982e-01,  3.2806e-02,\n                        1.8774e-01, -1.1591e-01, -5.9021e-02,  2.8702e-02, -3.8379e-01,\n                        1.9470e-01,  2.2986e-01,  1.3220e-01,  1.9446e-01,  3.7292e-02,\n                        1.5076e-01, -6.5308e-02, -3.5461e-02, -7.5867e-02,  1.3379e-01,\n                        1.0284e-01, -2.7740e-02, -4.9324e-03,  1.3000e-01, -9.5032e-02,\n                        2.3041e-02,  4.3884e-02,  1.4294e-01, -5.5054e-02,  6.8909e-02,\n                        5.5939e-02,  6.2408e-02, -2.1582e-01,  2.5000e-01,  5.3076e-01,\n                       -2.2791e-01, -2.5049e-01,  1.6138e-01, -9.7229e-02, -2.1667e-01,\n                       -2.4976e-01, -2.5146e-01, -5.2643e-02, -1.2457e-01, -7.0229e-03,\n                       -1.5002e-01, -8.4473e-02, -9.4681e-03,  8.1055e-02,  9.6252e-02,\n                        1.7773e-01, -8.1604e-02,  1.3806e-01, -1.8677e-01,  1.6895e-01,\n                       -3.7628e-02,  5.9662e-02, -9.2773e-02, -7.1533e-02, -1.9885e-01,\n                        5.9174e-02,  2.5000e-01, -2.3514e-02,  7.3608e-02,  2.2290e-01,\n                        6.6772e-02, -1.2500e-01, -1.3550e-01, -6.0028e-02, -1.3147e-01,\n                       -1.2756e-01,  6.3416e-02,  1.3062e-01,  1.2488e-01,  6.1584e-02,\n                       -6.7101e-03, -3.3875e-02, -1.6556e-02,  1.8738e-02, -4.7836e-03,\n                       -3.2776e-02,  1.4427e-02, -5.1221e-01, -4.6661e-02,  3.5343e-03,\n                        1.5762e-02,  2.3918e-03, -3.1250e-02, -1.8661e-02, -3.0685e-02,\n                        1.5762e-02, -8.2779e-03, -7.0190e-02,  1.4915e-02, -1.6663e-02,\n                        2.6138e-02, -8.9645e-03,  1.2150e-03, -1.5222e-01, -2.7023e-02,\n                       -2.4277e-02, -8.3237e-03,  2.5192e-02, -7.4120e-03, -7.1487e-03,\n                       -3.6377e-01,  7.0496e-02, -2.0630e-01,  2.1744e-02,  2.2049e-02,\n                       -1.7487e-02, -5.1910e-02,  3.1799e-02,  1.8585e-02,  2.3880e-02,\n                       -1.8433e-02,  9.0866e-03,  1.8753e-02,  1.7273e-02, -4.1580e-03,\n                        1.5976e-02, -2.8519e-02,  4.3030e-03, -2.9312e-02,  3.5217e-02,\n                        4.0771e-02, -1.8188e-01,  9.0103e-03,  5.0586e-01,  2.0020e-02,\n                        1.9806e-02,  1.6861e-02,  2.5436e-02,  1.7548e-02, -8.7357e-03,\n                       -1.6785e-02,  1.8906e-02, -1.3695e-02], device='cuda:0',\n                      dtype=torch.float16))]),\n '_non_persistent_buffers_set': set(),\n '_backward_pre_hooks': OrderedDict(),\n '_backward_hooks': OrderedDict(),\n '_is_full_backward_hook': None,\n '_forward_hooks': OrderedDict(),\n '_forward_hooks_with_kwargs': OrderedDict(),\n '_forward_pre_hooks': OrderedDict(),\n '_forward_pre_hooks_with_kwargs': OrderedDict(),\n '_state_dict_hooks': OrderedDict(),\n '_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_pre_hooks': OrderedDict(),\n '_load_state_dict_post_hooks': OrderedDict(),\n '_modules': OrderedDict(),\n 'infeatures': 768,\n 'outfeatures': 768,\n 'bits': 4,\n 'group_size': 128,\n 'maxq': 15,\n 'half_indim': 384,\n 'use_cuda_fp16': True,\n 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n 'kernel_switch_threshold': 128,\n 'autogptq_cuda_available': False,\n 'autogptq_cuda': None,\n 'trainable': False,\n 'device': device(type='cuda', index=0)}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"text = \"Hello my name is\"\ninputs = tokenizer(\n        text,\n    return_tensors = \"pt\"\n).to(0)\noutput = model.generate(**inputs)\nprint(tokenizer.decode(output[0], skip_special_tokens=True ))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:07:15.760742Z","iopub.execute_input":"2023-09-10T15:07:15.761726Z","iopub.status.idle":"2023-09-10T15:07:16.867276Z","shell.execute_reply.started":"2023-09-10T15:07:15.761676Z","shell.execute_reply":"2023-09-10T15:07:16.866304Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Hello my name is Kari and I am a student at the University of California, San Diego\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Quantization by passing custom dataset","metadata":{}},{"cell_type":"code","source":"model_id = \"facebook/opt-125m\"\nquantization_config = GPTQConfig(\n    bits = 4,\n    group_size = 128,\n    desc_act=False,\n    dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = quantization_config,\n    torch_dtype = torch.float16,\n    device_map = 'auto'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:11:54.234762Z","iopub.execute_input":"2023-09-10T15:11:54.235137Z","iopub.status.idle":"2023-09-10T15:12:41.174788Z","shell.execute_reply.started":"2023-09-10T15:11:54.235107Z","shell.execute_reply":"2023-09-10T15:12:41.173676Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f2c8ddbd60d44b3a8c22ac8a6f17ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"text = \"My name is\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\n\nout = model.generate(**inputs)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:12:48.366920Z","iopub.execute_input":"2023-09-10T15:12:48.367332Z","iopub.status.idle":"2023-09-10T15:12:48.865805Z","shell.execute_reply.started":"2023-09-10T15:12:48.367299Z","shell.execute_reply":"2023-09-10T15:12:48.864236Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"My name is a bit of a bit of a bit of a bit of a bit of a\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:13:34.208852Z","iopub.execute_input":"2023-09-10T15:13:34.209323Z","iopub.status.idle":"2023-09-10T15:13:34.252198Z","shell.execute_reply.started":"2023-09-10T15:13:34.209283Z","shell.execute_reply":"2023-09-10T15:13:34.251136Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbce98ff14a04dbfbb7829b473fa0709"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"opt-125m-gptq-4bit\")\ntokenizer.push_to_hub(\"opt-125m-gptq-4bit\")","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:14:14.652856Z","iopub.execute_input":"2023-09-10T15:14:14.653273Z","iopub.status.idle":"2023-09-10T15:14:19.818973Z","shell.execute_reply.started":"2023-09-10T15:14:14.653222Z","shell.execute_reply":"2023-09-10T15:14:19.817752Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/125M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4933926b4b47a8bc6e8b9deffa6cea"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AhmedElDokmak/opt-125m-gptq-4bit/commit/820bc23722693a355dac7e95c3a454424445c76b', commit_message='Upload tokenizer', commit_description='', oid='820bc23722693a355dac7e95c3a454424445c76b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading quantized model from the hub","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map = 'auto'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:18:42.353849Z","iopub.execute_input":"2023-09-10T15:18:42.354436Z","iopub.status.idle":"2023-09-10T15:19:20.728950Z","shell.execute_reply.started":"2023-09-10T15:18:42.354402Z","shell.execute_reply":"2023-09-10T15:19:20.728033Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09edbd9d26f94b6b81c422b953424412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbcaafeaeb72492faf01724ebdfce66a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6451d91959e4410d8a2fd16a6c82e482"}},"metadata":{}}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:19:28.019419Z","iopub.execute_input":"2023-09-10T15:19:28.019804Z","iopub.status.idle":"2023-09-10T15:19:28.030194Z","shell.execute_reply.started":"2023-09-10T15:19:28.019775Z","shell.execute_reply":"2023-09-10T15:19:28.028946Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (rotary_emb): LlamaRotaryEmbedding()\n          (k_proj): QuantLinear()\n          (o_proj): QuantLinear()\n          (q_proj): QuantLinear()\n          (v_proj): QuantLinear()\n        )\n        (mlp): LlamaMLP(\n          (act_fn): SiLUActivation()\n          (down_proj): QuantLinear()\n          (gate_proj): QuantLinear()\n          (up_proj): QuantLinear()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.quantization_config.to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:19:31.526416Z","iopub.execute_input":"2023-09-10T15:19:31.526893Z","iopub.status.idle":"2023-09-10T15:19:31.540366Z","shell.execute_reply.started":"2023-09-10T15:19:31.526853Z","shell.execute_reply":"2023-09-10T15:19:31.538039Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'quant_method': <QuantizationMethod.GPTQ: 'gptq'>,\n 'bits': 4,\n 'tokenizer': None,\n 'dataset': None,\n 'group_size': 128,\n 'damp_percent': 0.01,\n 'desc_act': False,\n 'sym': True,\n 'true_sequential': True,\n 'use_cuda_fp16': False,\n 'model_seqlen': None,\n 'block_name_to_quantize': None,\n 'module_name_preceding_first_block': None,\n 'batch_size': 1,\n 'pad_token_id': None,\n 'disable_exllama': False}"},"metadata":{}}]},{"cell_type":"code","source":"text = \"Hello my name is\"\ninputs = tokenizer(\n    text, \n    return_tensors=\"pt\"\n).to(0)\n\nout = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:20:14.076682Z","iopub.execute_input":"2023-09-10T15:20:14.077087Z","iopub.status.idle":"2023-09-10T15:20:52.400146Z","shell.execute_reply.started":"2023-09-10T15:20:14.077056Z","shell.execute_reply":"2023-09-10T15:20:52.399076Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Hello my name is Sarah and I am a 3rd year PhD student in the Department of Computer Science at the University of Cambridge. My research focuses on developing machine learning algorithms for medical image analysis, with a particular interest in brain imaging. I am super\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train quantized model using peft","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\nquantization_config = GPTQConfig(\n    bits = 4,\n    disable_exllama = True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = quantization_config,\n    device_map = 'auto'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:29:25.476671Z","iopub.execute_input":"2023-09-10T15:29:25.477052Z","iopub.status.idle":"2023-09-10T15:29:30.267919Z","shell.execute_reply.started":"2023-09-10T15:29:25.477019Z","shell.execute_reply":"2023-09-10T15:29:30.266813Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. disable_exllama, use_cuda_fp16) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.quantization_config.to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:29:44.333483Z","iopub.execute_input":"2023-09-10T15:29:44.333894Z","iopub.status.idle":"2023-09-10T15:29:44.341686Z","shell.execute_reply.started":"2023-09-10T15:29:44.333861Z","shell.execute_reply":"2023-09-10T15:29:44.340496Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'quant_method': <QuantizationMethod.GPTQ: 'gptq'>,\n 'bits': 4,\n 'tokenizer': None,\n 'dataset': None,\n 'group_size': 128,\n 'damp_percent': 0.01,\n 'desc_act': False,\n 'sym': True,\n 'true_sequential': True,\n 'use_cuda_fp16': False,\n 'model_seqlen': None,\n 'block_name_to_quantize': None,\n 'module_name_preceding_first_block': None,\n 'batch_size': 1,\n 'pad_token_id': None,\n 'disable_exllama': True}"},"metadata":{}}]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:30:08.795280Z","iopub.execute_input":"2023-09-10T15:30:08.795675Z","iopub.status.idle":"2023-09-10T15:30:08.806343Z","shell.execute_reply.started":"2023-09-10T15:30:08.795645Z","shell.execute_reply":"2023-09-10T15:30:08.805322Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r = 8,\n    lora_alpha = 32,\n    target_modules = [\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n    lora_dropout = 0.05,\n    bias = 'none',\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:33:25.791616Z","iopub.execute_input":"2023-09-10T15:33:25.792487Z","iopub.status.idle":"2023-09-10T15:33:26.061025Z","shell.execute_reply.started":"2023-09-10T15:33:25.792450Z","shell.execute_reply":"2023-09-10T15:33:26.059214Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"trainable params: 8,388,608 || all params: 270,798,848 || trainable%: 3.097726619575575\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:33:52.064806Z","iopub.execute_input":"2023-09-10T15:33:52.065180Z","iopub.status.idle":"2023-09-10T15:33:53.868389Z","shell.execute_reply.started":"2023-09-10T15:33:52.065150Z","shell.execute_reply":"2023-09-10T15:33:53.867327Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/Abirate--english_quotes to /root/.cache/huggingface/datasets/json/Abirate--english_quotes-7ef692ccb59fbf2a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33aa7574ff384ccfadf12d798749b97c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1045203deb946aba029ed49ad3c4963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ade2a59ac914a8c942d940e9fe42833"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/Abirate--english_quotes-7ef692ccb59fbf2a/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9038c1ef94a0482a8117883a4c676d2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677ed054ca5f47e59700c40a78d27aa7"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=2,\n        max_steps=10,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"adamw_hf\"\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\ntrainer = Trainer(\n    model,\n    train_dataset = data['train'],\n    args = training_args,\n    data_collator = data_collator\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:44:56.364267Z","iopub.execute_input":"2023-09-10T15:44:56.364693Z","iopub.status.idle":"2023-09-10T15:44:56.387828Z","shell.execute_reply.started":"2023-09-10T15:44:56.364661Z","shell.execute_reply":"2023-09-10T15:44:56.386517Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:44:57.860667Z","iopub.execute_input":"2023-09-10T15:44:57.861653Z","iopub.status.idle":"2023-09-10T15:46:06.930241Z","shell.execute_reply.started":"2023-09-10T15:44:57.861617Z","shell.execute_reply":"2023-09-10T15:46:06.929267Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 01:01, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.077400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.359600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.660200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.639100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.058100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.721200</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.149200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.123900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.898900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.049200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=2.1736684441566467, metrics={'train_runtime': 68.3464, 'train_samples_per_second': 0.585, 'train_steps_per_second': 0.146, 'total_flos': 1505696514048.0, 'train_loss': 2.1736684441566467, 'epoch': 0.02})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}