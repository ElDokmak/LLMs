{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JPS2VxViW9r1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None) :\n",
        "  \"\"\" Notations :\n",
        "          --> mask is for decoder only\n",
        "          --> k.transpose(-2,-1) as we are transposing the last 2 dims only\n",
        "          --> in softmax we used dim=-1 as we are applyin it to the last dim\n",
        "  \"\"\"\n",
        "  d_k = q.size()[-1]\n",
        "  scaled = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "  if mask is not None :\n",
        "    mask = torch.full(scaled.size(), float('-inf'))\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "    scaled += mask\n",
        "  attention = F.softmax(scaled, dim=-1)\n",
        "  values = torch.matmul(attention, v)\n",
        "\n",
        "  return values, attention\n"
      ],
      "metadata": {
        "id": "QshBEy32ZLT7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_Head_Attention(nn.Module) :\n",
        "  def __init__(self, input_dim, d_model, num_heads) :\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.qkv_layer = nn.Linear(input_dim, 3*d_model)\n",
        "    self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, x, mask = None) :\n",
        "    batch_size, seq_len, input_dim = x.size()\n",
        "    qkv = self.qkv_layer(x)\n",
        "    qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3*self.head_dim)\n",
        "    qkv = qkv.permute(0, 2, 1, 3)\n",
        "    q, k, v = qkv.chunk(3, dim=-1)\n",
        "    values, attention = scaled_dot_product(q, k, v, mask)\n",
        "    values = values.reshape(batch_size, seq_len, self.num_heads * self.head_dim)\n",
        "    out = self.linear_layer(values)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "nYlCkp7sbUVW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1024\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "batch_size = 30\n",
        "seq_len = 5\n",
        "\n",
        "x = torch.randn( (batch_size, seq_len, input_dim) )\n",
        "\n",
        "model = Multi_Head_Attention(input_dim, d_model, num_heads)\n",
        "out = model.forward(x)"
      ],
      "metadata": {
        "id": "RMySWlxWbcdH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QTMuOrJgdim",
        "outputId": "f95a43f6-cab4-42da-ef38-328c1defe5d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 5, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}