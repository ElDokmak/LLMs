# Transformers
<img src="https://miro.medium.com/v2/resize:fit:1400/1*10K7SmGoJ5zAtjkGfNfjkg.png">

## Content
* **[Embeddings](#Embeddings)**
* **Postitional Encoding**
* **Multi-Head Attention**
* **Layer Normalization**
* **Encoder**
* **Decoder**

## Embeddings
> Word Embedding can be thought of as a learned vector representation of each word.

> Neural networks learn through numbers so each word mapsto a vector with continuous values to represent that word.
<img align="center" width=300 src="https://miro.medium.com/v2/resize:fit:582/format:webp/0*6MnniQMOBPu4kFq3.png">
