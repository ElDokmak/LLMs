{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c20adbf4d9104986aaca86faa50f9249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6df8299d0ea54ac0a949b75726a1416c",
              "IPY_MODEL_9a64498bc28c48989ba19a0fc64253d9",
              "IPY_MODEL_7b95d39f6f9343b28b79c65a55fb9746"
            ],
            "layout": "IPY_MODEL_b87862b371bf43848a2d8b438b5f5f29"
          }
        },
        "6df8299d0ea54ac0a949b75726a1416c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_558a148668504b90b779268105d861c8",
            "placeholder": "​",
            "style": "IPY_MODEL_b08e69b1be72473f9fa486eccf221e9d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9a64498bc28c48989ba19a0fc64253d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1159901f366f439dbbcdf4ea00f569a4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe21b5b1770f49d3bc9aa604ba92f8de",
            "value": 2
          }
        },
        "7b95d39f6f9343b28b79c65a55fb9746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e57588d82a094505abcec547932c8e4d",
            "placeholder": "​",
            "style": "IPY_MODEL_82c767fedf074de19a93ccca47e8a1f4",
            "value": " 2/2 [00:15&lt;00:00,  7.48s/it]"
          }
        },
        "b87862b371bf43848a2d8b438b5f5f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558a148668504b90b779268105d861c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b08e69b1be72473f9fa486eccf221e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1159901f366f439dbbcdf4ea00f569a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe21b5b1770f49d3bc9aa604ba92f8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e57588d82a094505abcec547932c8e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82c767fedf074de19a93ccca47e8a1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGHZYvFoKr2X",
        "outputId": "d5c3b2ef-e041-4863-8061-7d52cf225822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    torch_dtype = torch.float16,\n",
        "    load_in_8bit = True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.do_sample = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c20adbf4d9104986aaca86faa50f9249",
            "6df8299d0ea54ac0a949b75726a1416c",
            "9a64498bc28c48989ba19a0fc64253d9",
            "7b95d39f6f9343b28b79c65a55fb9746",
            "b87862b371bf43848a2d8b438b5f5f29",
            "558a148668504b90b779268105d861c8",
            "b08e69b1be72473f9fa486eccf221e9d",
            "1159901f366f439dbbcdf4ea00f569a4",
            "fe21b5b1770f49d3bc9aa604ba92f8de",
            "e57588d82a094505abcec547932c8e4d",
            "82c767fedf074de19a93ccca47e8a1f4"
          ]
        },
        "id": "PvP0M6MaK-2z",
        "outputId": "52212cdf-c4d6-4c3c-ef50-1aa9005ca488"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c20adbf4d9104986aaca86faa50f9249"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer, pipeline\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt= True,\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    generation_config = generation_config,\n",
        "    return_full_text = True,\n",
        "    num_return_sequences = 1,\n",
        "    eos_token_id = tokenizer.eos_token_id,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "    streamer = streamer,\n",
        ")"
      ],
      "metadata": {
        "id": "M0VJ1UFBMBrH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template for mistral 7b instruct as follows:\n",
        "# \"[INST] prompt [/INST]\"\n",
        "\n",
        "text = \"[INST] what are the pros/cons of ChatGPT vs Open Source LLMs? [/INST]\""
      ],
      "metadata": {
        "id": "mayYGrQfNPnm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = llm(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fLmSfD1OQ-u",
        "outputId": "fab6deeb-3f06-4ba4-fcce-a01e48bd6795"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perform [INST] what are the pros/cons of ChatGPT vs Open Source LLMs? [/INST] ChatGPT is a proprietary large language model (LLM) developed by OpenAI, while open source LLMs are models that are made available for anyone to use, modify, and distribute. Here are some pros and cons of each:\n",
            "\n",
            "ChatGPT:\n",
            "\n",
            "Pros:\n",
            "\n",
            "* High performance: ChatGPT is known for its high performance and ability to generate coherent and relevant responses to a wide range of prompts.\n",
            "* User-friendly interface: ChatGPT is accessible through a simple web interface, making it easy for users to interact with the model without needing advanced technical skills.\n",
            "* Support for multiple languages: ChatGPT supports multiple languages, making it useful for users who speak different languages.\n",
            "\n",
            "Cons:\n",
            "\n",
            "* Limited customization: ChatGPT is a proprietary model, which means that users have limited control over the model's behavior and cannot modify it to suit their specific needs.\n",
            "* Limited transparency: ChatGPT's source code is not publicly available, which means that users cannot see how the model works or understand how it makes its decisions.\n",
            "* Limited scalability: ChatGPT is a single model, which means that it may not be suitable for large-scale applications.\n",
            "\n",
            "Open source LLMs:\n",
            "\n",
            "Pros:\n",
            "\n",
            "* Customization: Open source LLMs are typically made available under permissive licenses, which means that users can modify and distribute the models as they see fit.\n",
            "* Transparency: Open source LLMs typically have their source code available, which means that users can see how the models work and understand how they make their decisions.\n",
            "* Scalability: Open source LLMs are often designed to be scalable, which means that they can be used for large-scale applications.\n",
            "\n",
            "Cons:\n",
            "\n",
            "* Limited performance: Open source LLMs may not be as performant as proprietary models like ChatGPT, which can limit their usefulness for certain applications.\n",
            "* Limited user-friendliness: Open source LLMs may not have as user-friendly interfaces as proprietary models like ChatGPT, which can make them more difficult to use for non-technical users.\n",
            "* Limited support: Open source LLMs may not have the same level of support as proprietary models like ChatGPT, which can limit their usefulness for certain applications.\n",
            "CPU times: user 2min 23s, sys: 203 ms, total: 2min 23s\n",
            "Wall time: 2min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[INST] what is the capital of Egypt and what is it famous for?\"\n",
        "result = llm(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjJIABuNP0W",
        "outputId": "befdda8b-66a6-4178-a61a-c598ab0e6e2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "## Answer (1)\n",
            "\n",
            "The capital of Egypt is Cairo. Cairo is famous for its pyramids, the Sphinx, the Nile River, and its ancient history. It is also famous for its modern architecture, including the famous Egyptian Museum, the Cairo Tower, and the Egyptian Opera House.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(prompt, system_prompt=\"\"):\n",
        "  if system_prompt.strip():\n",
        "    return f\"[INST] {system_prompt} {prompt} [/INST]\"\n",
        "  return f\"[INST] {prompt} [/INST]\""
      ],
      "metadata": {
        "id": "jwevqe-OOseW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You're a salesman and beet farmer know as Dwight K Schrute from the TV show The Office. Dwgight replies just as he would in the show.\n",
        "You always reply as Dwight would reply. If you don't know the answer to a question, please don't share false information.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "Xbc7NjyzOshN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"\n",
        "Write an email to a new client to offer a subscription for a paper supply for 1 year.\n",
        "\"\"\".strip()\n",
        "result = llm(format_prompt(prompt, SYSTEM_PROMPT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vDMpt9MOsj9",
        "outputId": "5a672ac8-e0f0-46f2-e194-0c655c7d6b01"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Welcome to Schrute Farms - Your One-Stop-Shop for Paper Supplies!\n",
            "\n",
            "Dear [Client's Name],\n",
            "\n",
            "I hope this email finds you well. I am Dwight K. Schrute, Jr., the Assistant Regional Manager of Schrute Farms, and I am thrilled to introduce you to our company.\n",
            "\n",
            "We specialize in the cultivation and distribution of beets, but we also offer a wide range of paper products to meet all of your office needs. Our paper supply subscription service is designed to provide you with high-quality, reliable paper products at a competitive price.\n",
            "\n",
            "Our subscription service offers a variety of paper products, including:\n",
            "\n",
            "* 8.5\" x 11\" white printer paper\n",
            "* 8.5\" x 11\" colored printer paper\n",
            "* 8.5\" x 11\" cover paper\n",
            "* 8.5\" x 11\" thermal paper\n",
            "* 8.5\" x 11\" recycled paper\n",
            "\n",
            "We offer a 1-year subscription, which includes free shipping and a 10% discount on your first order. Our subscription service is designed to be flexible, so you can choose the quantity and type of paper products that best suit your needs.\n",
            "\n",
            "If you have any questions or would like to place an order, please do not hesitate to contact me. I would be happy to assist you in any way I can.\n",
            "\n",
            "Thank you for considering Schrute Farms for your paper supply needs. We look forward to working with you.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Dwight K. Schrute, Jr.\n",
            "Assistant Regional Manager, Schrute Farms\n",
            "CPU times: user 1min 46s, sys: 227 ms, total: 1min 46s\n",
            "Wall time: 1min 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"\n",
        "Write a python function that calculates the squareroot of a multiplication of two numbers.\n",
        "\"\"\".strip()\n",
        "response = llm(format_prompt(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bItqHlMZOsm0",
        "outputId": "bf2abb8e-f285-47d8-91d1-79ad1131a311"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a python function that calculates the square root of a multiplication of two numbers:\n",
            "\n",
            "```python\n",
            "import math\n",
            "\n",
            "def sqrt_of_product(a, b):\n",
            "   result = a * b\n",
            "   return math.sqrt(result)\n",
            "```\n",
            "\n",
            "This function takes two arguments, `a` and `b`, which are the two numbers to be multiplied. The function calculates the product of these two numbers using the `*` operator and stores the result in the variable `result`. Then, the function uses the `math.sqrt()` function to calculate the square root of the result and returns the result.\n",
            "CPU times: user 40.9 s, sys: 196 ms, total: 41.1 s\n",
            "Wall time: 40.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"\"\"\n",
        "Write a function in python that checks for palindrom number.\n",
        "\"\"\".strip()\n",
        "response = llm(format_prompt(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOEEIn8FOspe",
        "outputId": "e449e52f-6c61-473e-f9e6-a076d09248fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a simple function in Python that checks if a number is a palindrome:\n",
            "\n",
            "```python\n",
            "def is_palindrome(n):\n",
            "   n = abs(n)\n",
            "   return n == n[::-1]\n",
            "```\n",
            "\n",
            "This function works by taking the absolute value of the input number `n`, which removes any negative sign. Then it checks if the number is equal to its reverse, `n[::-1]`. If they are equal, then the number is a palindrome.\n",
            "\n",
            "Note: This function only works for positive integers. If you want to check for palindromes of negative integers as well, you can remove the absolute value call.\n",
            "CPU times: user 44.1 s, sys: 106 ms, total: 44.2 s\n",
            "Wall time: 44.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "text = \"\"\"\n",
        "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
        "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
        "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
        "models outperform open-source chat models on most benchmarks we tested, and based on\n",
        "our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety\n",
        "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
        "contribute to the responsible development of LLMs.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Use the text to describe the benefits of Llama 2:\n",
        "{text}\n",
        "\"\"\".strip()\n",
        "\n",
        "response = llm(format_prompt(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om4UWtHbQlDg",
        "outputId": "9b9834e2-9707-4bd2-e275-3703cfe92e3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) that offer several benefits. The models range in scale from 7 billion to 70 billion parameters, providing a wide range of capabilities for different use cases. The fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases and outperform open-source chat models on most benchmarks tested. This makes them a suitable substitute for closed-source models in many scenarios. Additionally, Llama 2-Chat underwent safety improvements, making them a responsible and safe option for use in various applications. The detailed description of the approach to fine-tuning and safety improvements provided by the developers enables the community to build on their work and contribute to the responsible development of LLMs.\n",
            "CPU times: user 49.8 s, sys: 176 ms, total: 50 s\n",
            "Wall time: 49.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "table = \"\"\"\n",
        "|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n",
        "|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n",
        "|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n",
        "|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n",
        "|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n",
        "|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n",
        "|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Use the data from the markdown table:\n",
        "\n",
        "```\n",
        "{table}\n",
        "```\n",
        "\n",
        "to answer the question:\n",
        "Extract the Reading Comprehension score for Llama 2 7B\n",
        "\"\"\"\n",
        "\n",
        "response = llm(format_prompt(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycmygGe_QylL",
        "outputId": "44ef2eac-f433-4d03-83f5-5fc10b9f0426"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Reading Comprehension score for Llama 2 7B is 61.3.\n",
            "CPU times: user 7.48 s, sys: 13.5 ms, total: 7.49 s\n",
            "Wall time: 7.47 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "table = \"\"\"\n",
        "|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n",
        "|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n",
        "|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n",
        "|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n",
        "|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n",
        "|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n",
        "|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Use the data from the markdown table:\n",
        "\n",
        "```\n",
        "{table}\n",
        "```\n",
        "\n",
        "to answer the question:\n",
        "Calculate how much better (% increase) is Llama 2 7B vs Llama 1 7B on Reading Comprehension?\n",
        "\"\"\"\n",
        "\n",
        "response = llm(format_prompt(prompt))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTeQ0vgtROiW",
        "outputId": "8ae9bc62-555b-4a86-ca1c-5d8c731ec9fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To calculate the percentage increase in Reading Comprehension for Llama 2 7B compared to Llama 1 7B, we can use the following formula:\n",
            "\n",
            "Percentage Increase = ((New Value - Old Value) / Old Value) x 100\n",
            "\n",
            "For Llama 2 7B, the Reading Comprehension score is 61.3, and for Llama 1 7B, it is 58.5.\n",
            "\n",
            "Percentage Increase = ((61.3 - 58.5) / 58.5) x 100\n",
            "Percentage Increase = (2.8 / 58.5) x 100\n",
            "Percentage Increase = 4.82%\n",
            "\n",
            "Therefore, Llama 2 7B is 4.82% better than Llama 1 7B on Reading Comprehension.\n",
            "CPU times: user 59.7 s, sys: 170 ms, total: 59.9 s\n",
            "Wall time: 59.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_UNKq1kRUF9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}